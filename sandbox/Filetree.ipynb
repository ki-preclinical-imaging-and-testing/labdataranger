{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122c0158-9a7e-48be-bb8f-1d0df3f5e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image\n",
    "from PIL.TiffTags import TAGS\n",
    "import pickle\n",
    "\n",
    "\n",
    "class FileTreeManager:\n",
    "    \n",
    "    def __init__(self, base_directory, skips=None, verbose=False, log_file=None, checkpoint_file=None):\n",
    "        \n",
    "        self.base_directory = Path(base_directory)\n",
    "        self.skips = skips if skips is not None else []\n",
    "        self.verbose = verbose\n",
    "        self.log_file = log_file\n",
    "        self.file_tree = None\n",
    "        self.file_types = (\n",
    "            '.log',\n",
    "            '.json',\n",
    "            '.xml',\n",
    "            '.xml.bak',\n",
    "            '.vxml',\n",
    "            '.vxml.bak',\n",
    "            '.mxml',\n",
    "            '.mxml.bak',\n",
    "            '.txt',\n",
    "            '.dcm',\n",
    "            '.dicom',\n",
    "            '.tif',\n",
    "            '.tiff'\n",
    "        )\n",
    "        \n",
    "        if log_file:\n",
    "            self.setup_logging(log_file, verbose)\n",
    "        \n",
    "        if checkpoint_file:\n",
    "            if Path(checkpoint_file).is_file():\n",
    "                self.load_state(checkpoint_file)\n",
    "                self.build_file_path_index()\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"No checkpoint available at {checkpoint_file}\")\n",
    "                    print(f\"To create, run `manager.collect_file_tree()`\")\n",
    "\n",
    "    \n",
    "    def setup_logging(self, log_file, verbose):\n",
    "        logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "        if verbose:\n",
    "            console = logging.StreamHandler()\n",
    "            console.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s %(message)s')\n",
    "            console.setFormatter(formatter)\n",
    "            logging.getLogger().addHandler(console)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        logging.info(message)\n",
    "\n",
    "    def remove_contents_key(self, meta):\n",
    "        if isinstance(meta, dict) and 'contents' in meta:\n",
    "            meta_copy = meta.copy()  # Create a shallow copy of the dictionary\n",
    "            del meta_copy['contents']  # Remove the 'contents' key\n",
    "            return meta_copy\n",
    "        return meta\n",
    "\n",
    "    def parse_metadata_file(self, file_path):\n",
    "        parsers = {\n",
    "            '.log': self.parse_log_file,\n",
    "            '.json': self.parse_json_file,\n",
    "            '.xml': self.parse_xml_file,\n",
    "            '.dcm': self.parse_dicom_file,\n",
    "            '.dicom': self.parse_dicom_file,\n",
    "            '.tif': self.parse_tif_file,\n",
    "            '.tiff': self.parse_tif_file,\n",
    "            # Add other specific file type parsers here\n",
    "        }\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        parser = parsers.get(ext)\n",
    "        if parser:\n",
    "            return parser(file_path)\n",
    "        else:\n",
    "            self.log_message(f\"No parser available for file with extension {ext}\")\n",
    "            return {}\n",
    "\n",
    "    # Dummy parser methods for demonstration\n",
    "    def parse_log_file(self, file_path):\n",
    "        meta_dict = {}\n",
    "        current_section = None\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line.startswith('[') and line.endswith(']'):\n",
    "                        current_section = line[1:-1]\n",
    "                        meta_dict[current_section] = {}\n",
    "                    elif '=' in line:\n",
    "                        key, value = line.split('=', 1)\n",
    "                        if current_section:\n",
    "                            meta_dict[current_section][key.strip()] = value.strip()\n",
    "                        else:\n",
    "                            meta_dict[key.strip()] = value.strip()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "        return meta_dict\n",
    "\n",
    "\n",
    "    def parse_json_file(self, file_path):\n",
    "        return {\"json\": \"metadata\"}\n",
    "\n",
    "    def parse_xml_file(self, file_path):\n",
    "        return {\"xml\": \"metadata\"}\n",
    "\n",
    "    def parse_dicom_file(self, file_path):\n",
    "        return {\"dicom\": \"metadata\"}\n",
    "\n",
    "    def parse_tif_file(self, file_path):\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            meta = {}\n",
    "            for key in img.tag_v2:\n",
    "                if key in TAGS:\n",
    "                    meta[TAGS[key]] = img.tag[key]\n",
    "                else:\n",
    "                    meta[f\"TAG_{key}\"] = img.tag[key]\n",
    "            return meta\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"Error processing TIFF file {file_path}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def update_folder_sizes(self, parts, file_size, tree):\n",
    "        current_tree = tree\n",
    "        for part in parts:\n",
    "            if part not in current_tree:\n",
    "                current_tree[part] = {'type': 'folder', 'size': 0, 'created': None, 'modified': None, 'contents': {}}\n",
    "            current_tree = current_tree[part]['contents']\n",
    "            if 'size' in current_tree:\n",
    "                current_tree['size'] += file_size\n",
    "            else:\n",
    "                current_tree['size'] = file_size\n",
    "\n",
    "    def collect_file_tree(self):\n",
    "        def add_to_tree(path, tree):\n",
    "            parts = path.relative_to(self.base_directory).parts\n",
    "            current_tree = tree\n",
    "            for part in parts[:-1]:\n",
    "                if part not in current_tree:\n",
    "                    current_tree[part] = {'type': 'folder', 'size': 0, 'created': None, 'modified': None, 'contents': {}}\n",
    "                current_tree = current_tree[part]['contents']\n",
    "            if path.is_dir():\n",
    "                stats = path.stat()\n",
    "                current_tree[parts[-1]] = {\n",
    "                    'type': 'folder',\n",
    "                    'size': 0,  # Placeholder size for folders\n",
    "                    'created': time.ctime(stats.st_ctime),\n",
    "                    'modified': time.ctime(stats.st_mtime),\n",
    "                    'contents': {}\n",
    "                }\n",
    "                self.log_message(f\"Added directory: {path}\")\n",
    "            elif path.is_file():\n",
    "                stats = path.stat()\n",
    "                meta_data = {}\n",
    "                if path.suffix in self.file_types:\n",
    "                    meta_data = self.parse_metadata_file(path)\n",
    "                current_tree[parts[-1]] = {\n",
    "                    'type': f'{path.suffix}',\n",
    "                    'size': stats.st_size,\n",
    "                    'created': time.ctime(stats.st_ctime),\n",
    "                    'modified': time.ctime(stats.st_mtime),\n",
    "                    'contents': None,\n",
    "                    'metadata': meta_data\n",
    "                }\n",
    "                self.log_message(f\"Added file: {path}\")\n",
    "                # Update the size of all parent directories\n",
    "                self.update_folder_sizes(parts[:-1], stats.st_size, tree)\n",
    "\n",
    "        # Ensure the base directory itself is included\n",
    "        file_tree = {\n",
    "            'base': {\n",
    "                'type': 'folder',\n",
    "                'size': 0,  # Placeholder size for base directory\n",
    "                'created': time.ctime(self.base_directory.stat().st_ctime),\n",
    "                'modified': time.ctime(self.base_directory.stat().st_mtime),\n",
    "                'contents': {}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for item in self.base_directory.rglob('*'):\n",
    "            if not any(skip in str(item) for skip in self.skips):\n",
    "                add_to_tree(item, file_tree['base']['contents'])\n",
    "                if item.is_dir():\n",
    "                    self.log_message(f\"Processing directory: {item}\")\n",
    "                elif item.is_file():\n",
    "                    self.log_message(f\"Processing file: {item}\")\n",
    "        \n",
    "        self.file_tree = file_tree\n",
    "        return file_tree\n",
    "\n",
    "    def build_graph_from_file_tree(self):\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        def add_nodes_and_edges(tree, parent_path):\n",
    "            for name, meta in tree.items():\n",
    "                current_path = f\"{parent_path}/{name}\" if parent_path else name\n",
    "                node_data = self.remove_contents_key(meta)\n",
    "                if isinstance(node_data, dict):\n",
    "                    G.add_node(current_path, **node_data)\n",
    "                    if parent_path:\n",
    "                        G.add_edge(parent_path, current_path)\n",
    "                    if meta['type'] == 'folder' and 'contents' in meta:\n",
    "                        add_nodes_and_edges(meta['contents'], current_path)\n",
    "                else:\n",
    "                    self.log_message(f\"Skipping node {current_path} with non-dict metadata: {node_data}\")\n",
    "        \n",
    "        add_nodes_and_edges(self.file_tree['base']['contents'], 'base')\n",
    "        return G\n",
    "\n",
    "    def build_file_path_index(self):\n",
    "        \"\"\" Build an index of all file paths for autocompletion. \"\"\"\n",
    "        self.file_path_index = []\n",
    "\n",
    "        def recurse_tree(tree, current_path):\n",
    "            for name, meta in tree.items():\n",
    "                if isinstance(meta, dict) and 'type' in meta:\n",
    "                    new_path = f\"{current_path}/{name}\" if current_path else name\n",
    "                    self.file_path_index.append(new_path)\n",
    "                    if meta['type'] == 'folder' and 'contents' in meta:\n",
    "                        recurse_tree(meta['contents'], new_path)\n",
    "\n",
    "        if self.file_tree:\n",
    "            recurse_tree(self.file_tree['base']['contents'], 'base')\n",
    "        print(\"File path index built.\")\n",
    "\n",
    "\n",
    "    def autocomplete_path(self, prefix):\n",
    "        \"\"\" Autocomplete potential directory paths based on the index. \"\"\"\n",
    "        return [path for path in self.file_path_index if path.startswith(prefix)]\n",
    "\n",
    "    def get_directory_contents(self, path):\n",
    "        parts = path.split('/')[1:]\n",
    "        current_tree = self.file_tree['base']['contents']\n",
    "        for part in parts:\n",
    "            if part in current_tree:\n",
    "                current_tree = current_tree[part]['contents']\n",
    "            else:\n",
    "                raise ValueError(f\"Path '{path}' not found in the directory structure.\")\n",
    "        return current_tree\n",
    "\n",
    "    def list_files(self, directory='base'):\n",
    "        _l = []\n",
    "\n",
    "        def process_files(tree):\n",
    "            for _k, _v in tree.items():\n",
    "                if isinstance(_v, dict) and _v.get('type') != 'folder':\n",
    "                    _d = _v.copy()\n",
    "                    del _d['contents']\n",
    "                    _d['name'] = _k\n",
    "                    _l.append(_d)\n",
    "\n",
    "        directory_contents = self.get_directory_contents(directory)\n",
    "        process_files(directory_contents)\n",
    "        return pd.DataFrame(_l)\n",
    "\n",
    "    def list_folders(self, directory='base'):\n",
    "        _l = []\n",
    "\n",
    "        def folder_size_sum(tree):\n",
    "            _sum_size = 0\n",
    "            for _i, _contents in tree.items():\n",
    "                if isinstance(_contents, dict):\n",
    "                    if _contents['type'] == 'folder':\n",
    "                        _sum_size += folder_size_sum(_contents['contents'])\n",
    "                    else:\n",
    "                        _sum_size += _contents['size']\n",
    "            return _sum_size\n",
    "\n",
    "        def process_folders(tree):\n",
    "            for _k, _v in tree.items():\n",
    "                if isinstance(_v, dict) and _v.get('type') == 'folder':\n",
    "                    _d = {\n",
    "                        'type': _v['type'],\n",
    "                        'size': folder_size_sum(_v['contents']),\n",
    "                        'created': _v['created'],\n",
    "                        'modified': _v['modified'],\n",
    "                        'name': _k\n",
    "                    }\n",
    "                    _l.append(_d)\n",
    "\n",
    "        directory_contents = self.get_directory_contents(directory)\n",
    "        process_folders(directory_contents)\n",
    "        return pd.DataFrame(_l)\n",
    "\n",
    "    def list_all(self, directory='base'):\n",
    "        files_df = self.list_files(directory)\n",
    "        folders_df = self.list_folders(directory)\n",
    "        return pd.concat([files_df, folders_df], ignore_index=True)\n",
    "\n",
    "    def extract_tiff_tags(self, img):\n",
    "        for _k in img.tag_v2:\n",
    "            if _k in TAGS.keys():\n",
    "                print(TAGS[_k], img.tag[_k])\n",
    "            else:\n",
    "                print(f\"CUSTOM TAG[{_k}]\", img.tag[_k])\n",
    "\n",
    "    def save_state(self, file_name):\n",
    "        \"\"\" Save the necessary data structures of the FileTreeManager to a file. \"\"\"\n",
    "        state = {\n",
    "            'base_directory': self.base_directory,\n",
    "            'file_tree': self.file_tree\n",
    "        }\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"State saved to {file_name}.\")\n",
    "\n",
    "    def load_state(self, file_name):\n",
    "        \"\"\" Load the necessary data structures of the FileTreeManager from a file. \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            self.base_directory = state['base_directory']\n",
    "            self.file_tree = state['file_tree']\n",
    "        print(f\"State loaded from {file_name}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec443e44-65dc-462f-a77f-e82b82e805d0",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "base_path = '/mnt/data/archive/LaraM_Full_Dataset'\n",
    "skips = [\n",
    "    'System Volume Information', \n",
    "    '$RECYCLE.BIN'\n",
    "]\n",
    "checkpoint_fstr = 'file_tree.pkl'\n",
    "log_fstr = 'file_tree.log'\n",
    "\n",
    "verbose = False\n",
    "base_dirs = glob.glob(f\"{base_path}/*\")\n",
    "for base_directory_path in base_dirs:\n",
    "    checkpoint_file = os.path.join(base_directory_path,checkpoint_fstr)\n",
    "    log_file = os.path.join(base_directory_path,log_fstr)\n",
    "    if verbose:\n",
    "        print(base_directory_path)\n",
    "        print(checkpoint_file)\n",
    "        print(log_file)\n",
    "        print()\n",
    "    manager = FileTreeManager(\n",
    "        base_directory_path,\n",
    "        skips, \n",
    "        log_file=log_file, \n",
    "        checkpoint_file=checkpoint_file\n",
    "    )\n",
    "    manager.collect_file_tree()\n",
    "    manager.save_state(checkpoint_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c140def-045e-40e7-a546-87848d1bebc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c52a0c-d1c3-4a51-9866-ae5bff208f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "# Define your FileTreeManager class here or import it if defined elsewhere\n",
    "\n",
    "base_path = '/mnt/data/archive/LaraM_Full_Dataset'\n",
    "skips = [\n",
    "    'System Volume Information', \n",
    "    '$RECYCLE.BIN'\n",
    "]\n",
    "checkpoint_fstr = 'file_tree_scrape.pkl'\n",
    "log_fstr = 'file_tree_scrape.log'\n",
    "\n",
    "verbose = False\n",
    "\n",
    "def process_directory(base_directory_path):\n",
    "    checkpoint_file = os.path.join(base_directory_path, checkpoint_fstr)\n",
    "    log_file = os.path.join(base_directory_path, log_fstr)\n",
    "    \n",
    "    if verbose:\n",
    "        print(base_directory_path)\n",
    "        print(checkpoint_file)\n",
    "        print(log_file)\n",
    "        print()\n",
    "    \n",
    "    manager = FileTreeManager(\n",
    "        base_directory_path,\n",
    "        skips, \n",
    "        log_file=log_file, \n",
    "        checkpoint_file=checkpoint_file\n",
    "    )\n",
    "    manager.collect_file_tree()\n",
    "    manager.save_state(checkpoint_file)\n",
    "    return base_directory_path\n",
    "\n",
    "def get_base_dirs(base_path):\n",
    "    base_dirs = []\n",
    "    for _dir in glob.glob(f\"{base_path}/*\"):\n",
    "        if Path(_dir).is_dir():\n",
    "            base_dirs.append(_dir)\n",
    "    return base_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f7fc85-2082-4f9f-a7fd-d2e9d0941e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dirs = get_base_dirs(base_path)\n",
    "len(base_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14007107-e34b-494e-a3c4-c691cbd84d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State loaded from /mnt/data/archive/LaraM_Full_Dataset/JnJ_MIT_uCTshare/file_tree_scrape.pkl.\n",
      "File path index built.\n",
      "State loaded from /mnt/data/archive/LaraM_Full_Dataset/$RECYCLE.BIN/file_tree_scrape.pkl.\n",
      "File path index built.\n",
      "State loaded from /mnt/data/archive/LaraM_Full_Dataset/System Volume Information/file_tree_scrape.pkl.\n",
      "File path index built.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/$RECYCLE.BIN/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/System Volume Information/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/JnJ_MIT_uCTshare/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1_post_Rec/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1.10.22_SW/file_tree_scrape.pkl.\n",
      "Error reading file /mnt/data/archive/LaraM_Full_Dataset/4.06.22_SW_U26/327/._327.log: 'utf-8' codec can't decode byte 0xb0 in position 37: invalid start byte\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/Sam_Normal mice/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.09.21_SW_U19/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/_Jnj/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.06.22_SW_U26/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.20.22_SW_U26(7)/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.06.21_SW_U19_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.1.22_sw/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/07.26.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/07.30.21_sw/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/070521/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.2.21_SW_USGI_U19/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.15.21_SW_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.19.21_SW_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.11.21_SW_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1.18.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.28.21_SW_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.29.21_SW_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U50/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/07.23.21_sw/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.21.22_SW_U30/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.07.21_SW_U16_USGI/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/05.11.22_SW_U26_U27/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U42_Florian/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.27.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U19_10.15.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/08.10.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U13/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.06.21_sw_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/08.18.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.05.2022_LM_U35_LLC/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.24.21_SW_USGI/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/6.6.22_SW_u26/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.16.21_SW_U19_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1.4.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/05.26.22_SW_U26/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/10.07.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/6.21.22_sw_u33/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.27.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U46/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1.3.22_SW_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/03.05.22_U25/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U48/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U11(04.22.21)/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.3.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.12.22_SW_U28/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U49/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.18.22_SW_u25/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1_pre_Rec/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/10.26.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.12.22_SW_U25/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.15.21_SW_U22_U23/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U44/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/03.03.22_SW_U20_u26/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/1.27.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U29_101522/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/03.17.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/Barium_U37_101222/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.23.21_SW_U19_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.15.22_U20_U25_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.3.22_740_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/20.12.21_SW_U20_U22_U23/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/04.16.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U12/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U9/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.25.22_SW_u29/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.14.21_SW_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U19_10.11.21_sw/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/10.21.21_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/10_31_2022_U29/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/3.28.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/11.18.21_SW_USGI/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/3.31.22_SW_u27/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.20.22_SW_U27/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U29_organoids_092122_5mo/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/05.09.22_SW_organoids/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/09.03.21_sw_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U47/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U16/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U37/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.12.22_SW_U27/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/6.9.22_SW_organoids/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/08.30.21_sw/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/5.3.22_SW_u31/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.24.22_SW_U25/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U10/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/5.12.22_SW_U32/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U39/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U14/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/04.06.22_SW_U27/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/03.02.22_SW/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/4.12.22_SW_U26/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/05.23.22_SW_organoids/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/12.09.21_SW_U22_U23_U18/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.09.22_SW_U25_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/02.21.22_SW_U25_U20/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/08.31.22_U34_LM/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U38/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U52/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/05.06.22_SW_U32/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/100522_U36_LLC_KP/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/u11/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U15/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U7/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U51/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U41/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U40/file_tree_scrape.pkl.\n",
      "State saved to /mnt/data/archive/LaraM_Full_Dataset/U17/file_tree_scrape.pkl.\n",
      "CPU times: user 58min 59s, sys: 20min 12s, total: 1h 19min 12s\n",
      "Wall time: 1h 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=56) as executor:\n",
    "\n",
    "    futures = {executor.submit(process_directory, base_dir): base_dir \n",
    "               for base_dir in base_dirs}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "    \n",
    "        base_dir = futures[future]\n",
    "        \n",
    "        try:\n",
    "            result = future.result()\n",
    "            if verbose:\n",
    "                print(f\"Processing completed for: {result}\")\n",
    "                \n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing {base_dir}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc10193-2d62-4ba7-8fee-d05e4c1099f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f138a37-d0d9-4e69-833b-b329b1356566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbb1327-6ce2-496d-b5f0-3d3c37a04e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State loaded from /mnt/data/archive/LaraM_Full_Dataset/U38/file_tree.pkl.\n",
      "File path index built.\n"
     ]
    }
   ],
   "source": [
    "log_file = os.path.join(base_dirs[0], 'file_tree_scrape.pkl')\n",
    "checkpoint_file = os.path.join(base_dirs[0], 'file_tree_scrape.pkl')\n",
    "manager = FileTreeManager(base_dirs[0], skips, checkpoint_file=checkpoint_file, log_file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b54e682-d947-47ff-b8a2-89b00ee74a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = manager.list_all('base/2022_10_24/Cage 6/1L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "840ccd90-17db-4237-8554-be3b1ddebf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>metadata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>.log</td>\n",
       "      <td>3390</td>\n",
       "      <td>Tue May 21 11:04:18 2024</td>\n",
       "      <td>Tue Oct 25 10:12:44 2022</td>\n",
       "      <td>{'log': 'metadata'}</td>\n",
       "      <td>1L.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  size                   created                  modified  \\\n",
       "125  .log  3390  Tue May 21 11:04:18 2024  Tue Oct 25 10:12:44 2022   \n",
       "\n",
       "                metadata    name  \n",
       "125  {'log': 'metadata'}  1L.log  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df[_df['type'] == '.log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "542576ca-75a1-419b-92af-b052f5e88273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Scanner=SkyScan1276\n",
      "Instrument S/N=18F17059\n",
      "Software Version=1.6\n",
      "Magnification Drive Version=3.4\n",
      "Camera Drive Version=3.4\n",
      "Filter  Drive Version=3.4\n",
      "Animal Drive Version=3.4\n",
      "Home Directory=C:\\SkyScan1276\n",
      "Source Type=HAMAMATSU L10321\n",
      "Camera Type=XIMEA MH110XC-KK-TP\n",
      "Camera Pixel Size (um)=17.359\n",
      "Camera X/Y Ratio=0.9949\n",
      "[User]\n",
      "User Name=qureshik\n",
      "Computer Name=MICROCT\n",
      "[Acquisition]\n",
      "Data Directory=D:\\Results\\LauraM\\U38\\2022_10_24\\Cage 6\\1L\n",
      "Filename Prefix=1L\n",
      "Number Of Files=  926\n",
      "Number Of Rows=  672\n",
      "Number Of Columns= 1008\n",
      "Filename Index Length=8\n",
      "Partial Width=OFF\n",
      "Image crop origin X=0\n",
      "Image crop origin Y=0\n",
      "Camera binning=4x4\n",
      "Image Rotation=-0.01000\n",
      "Optical Axis (line)=  346\n",
      "Object to Source (mm)=92.863\n",
      "Camera to Source (mm)=160.543\n",
      "Source Voltage (kV)= 100\n",
      "Source Current (uA)= 200\n",
      "Image Pixel Size (um)=40.164000\n",
      "Scaled Image Pixel Size (um)=40.164000\n",
      "Image Format=TIFF\n",
      "Depth (bits)=16\n",
      "Reference Intensity=55000\n",
      "Exposure (ms)=111\n",
      "Rotation Step (deg)=0.389\n",
      "Use 360 Rotation=YES\n",
      "Scanning position=247.000 mm\n",
      "Frame Averaging=OFF (1)\n",
      "Flat Field Correction=ON\n",
      "Random Movement=OFF (5)\n",
      "Filter=Al 0.5mm\n",
      "Filtering Applied=none\n",
      "Gantry direction=CC\n",
      "Rotation Direction=CC\n",
      "Intrinsic CS rotation in degree=180.0\n",
      "Type of Detector Motion=CONTINUOUS\n",
      "Image on demand=OFF\n",
      "Source spot size=Small\n",
      "FF updating interval=133\n",
      "Beam position=100\n",
      "Rotation speed=1\n",
      "Scanning Trajectory=ROUND\n",
      "Suggested HU - Calibration=180000\n",
      "Number Of Horizontal Offset Positions=1\n",
      "Dose estimation Air(mGy)=978\n",
      "Dose estimation Mouse(mGy)=557\n",
      "Dose estimation Rat(mGy)=270\n",
      "Study Date and Time=25 Oct 2022  10h:08m:23s\n",
      "Scan duration=0h:2m:4s\n",
      "Maximum vertical TS=5.0\n",
      "[Reconstruction]\n",
      "Reconstruction Program=NRecon\n",
      "Program Version=Version: 1.7.4.2\n",
      "Program Home Directory=C:\\SkyScan1276\n",
      "Reconstruction engine=GPUReconServer\n",
      "Engine version=Version: 1.7.4\n",
      "Reconstruction from batch=No\n",
      "Postalignment Applied=1\n",
      "Postalignment=0.50\n",
      "Reconstruction servers= MICROCT \n",
      "Dataset Origin=SkyScan1276\n",
      "Dataset Prefix=1L\n",
      "Dataset Directory=D:\\Results\\LauraM\\U38\\2022_10_24\\Cage 6\\1L\n",
      "Output Directory=D:\\Results\\LauraM\\U38\\2022_10_24\\Cage 6\\1L\\1L_Rec\n",
      "Time and Date=Oct 25, 2022  10:12:43\n",
      "First Section=258\n",
      "Last Section=664\n",
      "Reconstruction duration per slice (seconds)=0.174447\n",
      "Total reconstruction time (407 slices) in seconds=71.000000\n",
      "Section to Section Step=1\n",
      "Sections Count=407\n",
      "Result File Type=BMP\n",
      "Result File Header Length (bytes)=1134\n",
      "Result Image Width (pixels)=688\n",
      "Result Image Height (pixels)=688\n",
      "Pixel Size (um)=40.16400\n",
      "Reconstruction Angular Range (deg)=360.21\n",
      "Use 180+=OFF\n",
      "Angular Step (deg)=0.3890\n",
      "Smoothing=2\n",
      "Smoothing kernel=2 (Gaussian)\n",
      "Ring Artifact Correction=2\n",
      "Draw Scales=OFF\n",
      "Object Bigger than FOV=ON\n",
      "Reconstruction from ROI=ON\n",
      "ROI Top (pixels)=864\n",
      "ROI Bottom (pixels)=175\n",
      "ROI Left (pixels)=201\n",
      "ROI Right (pixels)=890\n",
      "ROI reference length=1008\n",
      "Filter cutoff relative to Nyquist frequency=100\n",
      "Filter type=0\n",
      "Filter type description=Hamming (Alpha=0.54)\n",
      "Undersampling factor=1\n",
      "Threshold for defect pixel mask (%)=0\n",
      "Beam Hardening Correction (%)=10\n",
      "CS Static Rotation (deg)=0.00\n",
      "CS Static Rotation Total(deg)=180.00\n",
      "Minimum for CS to Image Conversion=0.000000\n",
      "Maximum for CS to Image Conversion=0.015000\n",
      "HU Calibration=OFF\n",
      "BMP LUT=0\n",
      "Cone-beam Angle Horiz.(deg)=24.594404\n",
      "Cone-beam Angle Vert.(deg)=16.536995\n"
     ]
    }
   ],
   "source": [
    "!cat /mnt/data/archive/LaraM_Full_Dataset/U38/2022_10_24/Cage\\ 6/1L/1L.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee644a9-3efe-42fd-bb1d-1933052ca4e5",
   "metadata": {},
   "source": [
    "Now that this seems to be working, I need something to find all of the .pkl files, export them as graphs, and then write them correctly to a knowledge graph database.\n",
    "\n",
    "Note: If this becomes too cumbersome, there is probably a nice way to \"compress\" metadata on Tiff stacks so that redundant information isn't repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20e28f1-3bec-4a4e-9d4c-a94d3704ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State loaded from file_tree.pkl.\n",
      "File path index built.\n"
     ]
    }
   ],
   "source": [
    "base_directory_path = '/mnt/data/archive/LaraM_Full_Dataset/U10'\n",
    "# base_directory_path = '/mnt/data/archive/LaraM_Full_Dataset/02.27.22_SW/'\n",
    "skips = [\n",
    "    'System Volume Information', \n",
    "    '$RECYCLE.BIN'\n",
    "]\n",
    "checkpoint_file = 'file_tree.pkl'\n",
    "log_file = 'file_tree.log'\n",
    "\n",
    "manager = FileTreeManager(\n",
    "    base_directory_path,\n",
    "    skips, \n",
    "    log_file=log_file, \n",
    "    checkpoint_file=checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b02a91-8f29-4783-8068-b3ab0e846105",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = manager.autocomplete_path('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a01844-e253-4b64-91df-70d01844b7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>folder</td>\n",
       "      <td>2857577690</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:44:44 2021</td>\n",
       "      <td>Cage 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>folder</td>\n",
       "      <td>4751325695</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:35:04 2021</td>\n",
       "      <td>Cage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>folder</td>\n",
       "      <td>3837622281</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:39:32 2021</td>\n",
       "      <td>Cage 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>folder</td>\n",
       "      <td>4812148812</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:41:24 2021</td>\n",
       "      <td>Cage 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>folder</td>\n",
       "      <td>2970612415</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:38:04 2021</td>\n",
       "      <td>Cage 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>folder</td>\n",
       "      <td>4846813889</td>\n",
       "      <td>Tue May 21 11:04:28 2024</td>\n",
       "      <td>Sun Mar 21 13:36:58 2021</td>\n",
       "      <td>Cage 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type        size                   created                  modified  \\\n",
       "0  folder  2857577690  Tue May 21 11:04:28 2024  Sun Mar 21 13:44:44 2021   \n",
       "1  folder  4751325695  Tue May 21 11:04:28 2024  Sun Mar 21 13:35:04 2021   \n",
       "2  folder  3837622281  Tue May 21 11:04:28 2024  Sun Mar 21 13:39:32 2021   \n",
       "3  folder  4812148812  Tue May 21 11:04:28 2024  Sun Mar 21 13:41:24 2021   \n",
       "4  folder  2970612415  Tue May 21 11:04:28 2024  Sun Mar 21 13:38:04 2021   \n",
       "5  folder  4846813889  Tue May 21 11:04:28 2024  Sun Mar 21 13:36:58 2021   \n",
       "\n",
       "     name  \n",
       "0  Cage 6  \n",
       "1  Cage 1  \n",
       "2  Cage 4  \n",
       "3  Cage 5  \n",
       "4  Cage 3  \n",
       "5  Cage 2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.list_all(\"base/2021_0321\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd63a5ea-4295-4797-93e1-2137b3fb94e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>metadata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:30 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000296.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:30 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000251.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:32 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000422.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:30 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000291.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:28 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000117.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:32 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000426.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:26 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000057.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:26 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000066.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>.tif</td>\n",
       "      <td>1355230</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 18:24:32 2021</td>\n",
       "      <td>{'ImageWidth': (1008,), 'ImageLength': (672,),...</td>\n",
       "      <td>2R00000449.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>folder</td>\n",
       "      <td>157722442</td>\n",
       "      <td>Tue May 21 11:04:29 2024</td>\n",
       "      <td>Tue Mar 16 19:01:14 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2R_Rec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type       size                   created                  modified  \\\n",
       "0      .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:30 2021   \n",
       "1      .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:30 2021   \n",
       "2      .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:32 2021   \n",
       "3      .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:30 2021   \n",
       "4      .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:28 2021   \n",
       "..      ...        ...                       ...                       ...   \n",
       "572    .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:32 2021   \n",
       "573    .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:26 2021   \n",
       "574    .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:26 2021   \n",
       "575    .tif    1355230  Tue May 21 11:04:29 2024  Tue Mar 16 18:24:32 2021   \n",
       "576  folder  157722442  Tue May 21 11:04:29 2024  Tue Mar 16 19:01:14 2021   \n",
       "\n",
       "                                              metadata            name  \n",
       "0    {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000296.tif  \n",
       "1    {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000251.tif  \n",
       "2    {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000422.tif  \n",
       "3    {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000291.tif  \n",
       "4    {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000117.tif  \n",
       "..                                                 ...             ...  \n",
       "572  {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000426.tif  \n",
       "573  {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000057.tif  \n",
       "574  {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000066.tif  \n",
       "575  {'ImageWidth': (1008,), 'ImageLength': (672,),...  2R00000449.tif  \n",
       "576                                                NaN          2R_Rec  \n",
       "\n",
       "[577 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.list_all(\"base/2021_0316/Cage 4/2R/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbef40f2-b5a6-406e-83c8-12bfed9a8315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Cage 6', 'Cage 1', 'Cage 4', 'Cage 5', 'Cage 3', 'Cage 2', 'size'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2bcbf76-6367-49da-8924-7ec3ca7c9bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'size', 'created', 'modified', 'contents'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents['Cage 6'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e0177-8f07-4441-8889-e2e78869840a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
